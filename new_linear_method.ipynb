{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56e97315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Actual Data Loaded ---\n",
      "Train samples: 8000, Test samples: 2000\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- 1. DATA LOADING (Using uploaded files: train.csv and test.csv) ---\n",
    "\n",
    "try:\n",
    "    # Load the actual provided files\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "\n",
    "    print(\"--- Actual Data Loaded ---\")\n",
    "    print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Fallback/Error handling if files are not accessible or named differently\n",
    "    print(\"Error: train.csv or test.csv not found.\")\n",
    "    print(\"Please ensure the files are correctly named and accessible.\")\n",
    "    # Exiting gracefully if the files cannot be loaded\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Store the test IDs for the final submission file\n",
    "test_ids = test_df['Id']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train = train_df.drop(['Id', 'Recovery Index'], axis=1)\n",
    "y_train = train_df['Recovery Index']\n",
    "X_test = test_df.drop('Id', axis=1)\n",
    "\n",
    "\n",
    "# --- 2. PREPROCESSING AND FEATURE ENGINEERING ---\n",
    "\n",
    "def preprocess_and_engineer_features(df):\n",
    "    \"\"\"Applies categorical encoding and creates new linearly combined features.\"\"\"\n",
    "\n",
    "    # 1. Categorical Encoding (Lifestyle Activities: Yes -> 1, No -> 0)\n",
    "    # Ensure there are no NaNs in this column before mapping\n",
    "    df['Lifestyle Activities'] = df['Lifestyle Activities'].fillna('No')\n",
    "    df['Lifestyle_Active'] = df['Lifestyle Activities'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    df = df.drop('Lifestyle Activities', axis=1)\n",
    "\n",
    "    # 2. Linear Feature Combinations (Avoiding polynomials/high order)\n",
    "    # The new features are derived from linear operations (multiplication, division)\n",
    "    # of the base features, which maintains the spirit of linear modeling when \n",
    "    # the new features are treated as independent inputs.\n",
    "\n",
    "    T = df['Therapy Hours']\n",
    "    H = df['Initial Health Score']\n",
    "    S = df['Average Sleep Hours']\n",
    "    F = df['Follow-Up Sessions']\n",
    "\n",
    "    # Handle potential division by zero for stability\n",
    "    epsilon = 1e-6 # Small constant to prevent division by zero\n",
    "\n",
    "    # Ratio: Therapy hours per effective session count\n",
    "    # Represents the intensity/efficiency of therapy hours relative to follow-up commitment\n",
    "    df['Therapy_Per_FollowUp'] = T / (F + epsilon)\n",
    "\n",
    "    # Product: Combined health momentum (Sleep quality * Initial condition)\n",
    "    df['Sleep_Health_Product'] = S * H\n",
    "\n",
    "    # Sum: Total effort in treatment\n",
    "    # Simple aggregation of time/commitment resources\n",
    "    df['Combined_Treatment_Hours'] = T + F\n",
    "\n",
    "    # Ratio: Health score per hour of sleep (Efficiency of rest)\n",
    "    df['Health_Per_Sleep'] = H / (S + epsilon)\n",
    "\n",
    "    # Product: Direct Therapy Impact Score (Therapy hours * Initial condition)\n",
    "    df['Therapy_Health_Product'] = T * H\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing and feature engineering to both datasets\n",
    "X_train_processed = preprocess_and_engineer_features(X_train.copy())\n",
    "X_test_processed = preprocess_and_engineer_features(X_test.copy())\n",
    "\n",
    "# Ensure columns are consistent after processing\n",
    "# This is crucial in production to handle cases where one set might miss a category (not applicable here)\n",
    "# or if columns were added/removed differently.\n",
    "X_test_processed = X_test_processed[X_train_processed.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb35e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing OOF for ridge ...\n",
      "ridge OOF RMSE: 2.0441\n",
      "Computing OOF for elasticnet ...\n",
      "elasticnet OOF RMSE: 2.0476\n",
      "Computing OOF for hgb ...\n",
      "hgb OOF RMSE: 2.2307\n",
      "OOF compute finished in 7.4s\n",
      "Base models OOF RMSEs: {'ridge': 2.0441044578324297, 'elasticnet': 2.0475641538575147, 'hgb': 2.2306827053024123}\n",
      "Stacking meta OOF RMSE: 2.0437\n",
      "Weighted ensemble OOF RMSE: 2.0652\n",
      "Saved submission_ensemble_improved.csv — first rows:\n",
      "     Id  Recovery Index\n",
      "0  6253              55\n",
      "1  4685              23\n",
      "2  1732              48\n",
      "3  4743              31\n",
      "4  4522              43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ensemble_models.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# X_train_processed, X_test_processed, y_train, test_ids are expected above\n",
    "X = X_train_processed.copy()\n",
    "X_test = X_test_processed.copy()\n",
    "y = y_train.values.ravel()\n",
    "\n",
    "rng_seed = 42\n",
    "# cross_val_predict requires a partitioning CV (no repeats) so use KFold with shuffle\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=rng_seed)\n",
    "\n",
    "# Ensure no NaNs remain (impute with median for numeric)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = pd.DataFrame(num_imputer.fit_transform(X), columns=X.columns, index=X.index)\n",
    "X_test_imputed = pd.DataFrame(num_imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "n_features = X_imputed.shape[1]\n",
    "k_select = min(12, max(3, n_features))\n",
    "\n",
    "models = {\n",
    "    \"ridge\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"select\", SelectKBest(score_func=f_regression, k=k_select)),\n",
    "        (\"ridge\", RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 50.0], cv=5))\n",
    "    ]),\n",
    "    \"elasticnet\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"select\", SelectKBest(score_func=f_regression, k=k_select)),\n",
    "        (\"en\", ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9], alphas=[0.01, 0.1, 1.0], cv=5, random_state=rng_seed, max_iter=5000))\n",
    "    ]),\n",
    "    \"hgb\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"hgb\", HistGradientBoostingRegressor(random_state=rng_seed, max_iter=800, learning_rate=0.05))\n",
    "    ])\n",
    "}\n",
    "\n",
    "oof_dict = {}\n",
    "rmse_dict = {}\n",
    "start = time.time()\n",
    "for name, pipe in models.items():\n",
    "    print(f\"Computing OOF for {name} ...\")\n",
    "    # use X_imputed.values so pipeline receives numpy array consistently\n",
    "    oof = cross_val_predict(pipe, X_imputed.values, y, cv=cv, n_jobs=-1, method=\"predict\")\n",
    "    rmse = np.sqrt(mean_squared_error(y, oof))\n",
    "    oof_dict[name] = oof\n",
    "    rmse_dict[name] = rmse\n",
    "    print(f\"{name} OOF RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(f\"OOF compute finished in {time.time()-start:.1f}s\")\n",
    "\n",
    "# Build OOF matrix for stacking\n",
    "oof_matrix = np.vstack([oof_dict[m] for m in oof_dict]).T\n",
    "print(\"Base models OOF RMSEs:\", rmse_dict)\n",
    "\n",
    "# Stacking meta-model (train on OOF predictions)\n",
    "meta = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5)\n",
    "meta.fit(oof_matrix, y)\n",
    "oof_meta_pred = meta.predict(oof_matrix)\n",
    "rmse_meta = np.sqrt(mean_squared_error(y, oof_meta_pred))\n",
    "print(f\"Stacking meta OOF RMSE: {rmse_meta:.4f}\")\n",
    "\n",
    "# Fit base models on full training data and produce test predictions\n",
    "test_preds_models = []\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_imputed.values, y)\n",
    "    preds = pipe.predict(X_test_imputed.values)\n",
    "    test_preds_models.append(preds)\n",
    "test_matrix = np.vstack(test_preds_models).T\n",
    "\n",
    "# Use stacking meta to combine test predictions\n",
    "test_preds_stack = meta.predict(test_matrix)\n",
    "\n",
    "# As a fallback also compute simple weighted average (inverse-rmse^2) and compare\n",
    "rmse_vals = np.array([rmse_dict[m] for m in oof_dict])\n",
    "weights = 1.0 / (rmse_vals**2 + 1e-12)\n",
    "weights = weights / weights.sum()\n",
    "test_preds_weighted = (test_matrix * weights).sum(axis=1)\n",
    "\n",
    "# Choose final test preds: stacking (preferred) but also print both RMSE on OOF proxy\n",
    "oof_weighted = (oof_matrix * weights).sum(axis=1)\n",
    "rmse_ens_weighted = np.sqrt(mean_squared_error(y, oof_weighted))\n",
    "print(f\"Weighted ensemble OOF RMSE: {rmse_ens_weighted:.4f}\")\n",
    "\n",
    "final_test_preds = test_preds_stack  # stacking tends to perform better\n",
    "\n",
    "# Clip to training target range; do NOT aggressively round before submission (rounding can increase RMSE)\n",
    "y_min, y_max = y.min(), y.max()\n",
    "final_test_preds = np.clip(final_test_preds, y_min, y_max)\n",
    "\n",
    "# If the competition requires integer targets, round at submission; otherwise keep float.\n",
    "# Keep as integer only if necessary:\n",
    "final_test_preds_int = np.rint(final_test_preds).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"Recovery Index\": final_test_preds_int\n",
    "})\n",
    "submission.to_csv(\"submission_ensemble_improved.csv\", index=False)\n",
    "print(\"Saved submission_ensemble_improved.csv — first rows:\")\n",
    "print(submission.head())\n",
    "\n",
    "import joblib\n",
    "joblib.dump({\"models\": models, \"meta\": meta, \"weights\": weights, \"rmse\": rmse_dict}, \"ensemble_models.joblib\", compress=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702a667",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
